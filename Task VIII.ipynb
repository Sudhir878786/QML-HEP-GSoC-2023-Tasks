{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=depth)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        x = self.patch_to_embedding(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.transformer(x, mask)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DOWNLOAD_PATH = '/data/mnist'\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 1000\n",
    "\n",
    "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
    "                                       transform=transform_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
    "                                      transform=transform_mnist)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_history):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = F.log_softmax(model(data), dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "            loss_history.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = F.log_softmax(model(data), dim=1)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append(avg_loss)\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "          '{:5}'.format(total_samples) + ' (' +\n",
    "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.3175\n",
      "[10000/60000 ( 17%)]  Loss: 0.4158\n",
      "[20000/60000 ( 33%)]  Loss: 0.3542\n",
      "[30000/60000 ( 50%)]  Loss: 0.1210\n",
      "[40000/60000 ( 67%)]  Loss: 0.3141\n",
      "[50000/60000 ( 83%)]  Loss: 0.1689\n",
      "\n",
      "Average test loss: 0.2066  Accuracy: 9375/10000 (93.75%)\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.3268\n",
      "[10000/60000 ( 17%)]  Loss: 0.1322\n",
      "[20000/60000 ( 33%)]  Loss: 0.1753\n",
      "[30000/60000 ( 50%)]  Loss: 0.1520\n",
      "[40000/60000 ( 67%)]  Loss: 0.3665\n",
      "[50000/60000 ( 83%)]  Loss: 0.2329\n",
      "\n",
      "Average test loss: 0.1975  Accuracy: 9420/10000 (94.20%)\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.2030\n",
      "[10000/60000 ( 17%)]  Loss: 0.1219\n",
      "[20000/60000 ( 33%)]  Loss: 0.2226\n",
      "[30000/60000 ( 50%)]  Loss: 0.1727\n",
      "[40000/60000 ( 67%)]  Loss: 0.2231\n",
      "[50000/60000 ( 83%)]  Loss: 0.2227\n",
      "\n",
      "Average test loss: 0.1246  Accuracy: 9631/10000 (96.31%)\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.0949\n",
      "[10000/60000 ( 17%)]  Loss: 0.2173\n",
      "[20000/60000 ( 33%)]  Loss: 0.2673\n",
      "[30000/60000 ( 50%)]  Loss: 0.1803\n",
      "[40000/60000 ( 67%)]  Loss: 0.1890\n",
      "[50000/60000 ( 83%)]  Loss: 0.1769\n",
      "\n",
      "Average test loss: 0.1571  Accuracy: 9538/10000 (95.38%)\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.1606\n",
      "[10000/60000 ( 17%)]  Loss: 0.2070\n",
      "[20000/60000 ( 33%)]  Loss: 0.1010\n",
      "[30000/60000 ( 50%)]  Loss: 0.2170\n",
      "[40000/60000 ( 67%)]  Loss: 0.1973\n",
      "[50000/60000 ( 83%)]  Loss: 0.1540\n",
      "\n",
      "Average test loss: 0.1171  Accuracy: 9633/10000 (96.33%)\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.0808\n",
      "[10000/60000 ( 17%)]  Loss: 0.1757\n",
      "[20000/60000 ( 33%)]  Loss: 0.1005\n",
      "[30000/60000 ( 50%)]  Loss: 0.2104\n",
      "[40000/60000 ( 67%)]  Loss: 0.3347\n",
      "[50000/60000 ( 83%)]  Loss: 0.1879\n",
      "\n",
      "Average test loss: 0.1247  Accuracy: 9629/10000 (96.29%)\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.1342\n",
      "[10000/60000 ( 17%)]  Loss: 0.0693\n",
      "[20000/60000 ( 33%)]  Loss: 0.2126\n",
      "[30000/60000 ( 50%)]  Loss: 0.2196\n",
      "[40000/60000 ( 67%)]  Loss: 0.1379\n",
      "[50000/60000 ( 83%)]  Loss: 0.1429\n",
      "\n",
      "Average test loss: 0.1226  Accuracy: 9622/10000 (96.22%)\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.1358\n",
      "[10000/60000 ( 17%)]  Loss: 0.1895\n",
      "[20000/60000 ( 33%)]  Loss: 0.1287\n",
      "[30000/60000 ( 50%)]  Loss: 0.0835\n",
      "[40000/60000 ( 67%)]  Loss: 0.1054\n",
      "[50000/60000 ( 83%)]  Loss: 0.1245\n",
      "\n",
      "Average test loss: 0.1023  Accuracy: 9695/10000 (96.95%)\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.1902\n",
      "[10000/60000 ( 17%)]  Loss: 0.1602\n",
      "[20000/60000 ( 33%)]  Loss: 0.1419\n",
      "[30000/60000 ( 50%)]  Loss: 0.1101\n",
      "[40000/60000 ( 67%)]  Loss: 0.2098\n",
      "[50000/60000 ( 83%)]  Loss: 0.1019\n",
      "\n",
      "Average test loss: 0.0963  Accuracy: 9723/10000 (97.23%)\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0981\n",
      "[10000/60000 ( 17%)]  Loss: 0.1781\n",
      "[20000/60000 ( 33%)]  Loss: 0.0460\n",
      "[30000/60000 ( 50%)]  Loss: 0.1539\n",
      "[40000/60000 ( 67%)]  Loss: 0.0796\n",
      "[50000/60000 ( 83%)]  Loss: 0.1383\n",
      "\n",
      "Average test loss: 0.1073  Accuracy: 9684/10000 (96.84%)\n",
      "\n",
      "Epoch: 11\n",
      "[    0/60000 (  0%)]  Loss: 0.1278\n",
      "[10000/60000 ( 17%)]  Loss: 0.2842\n",
      "[20000/60000 ( 33%)]  Loss: 0.0962\n",
      "[30000/60000 ( 50%)]  Loss: 0.0701\n",
      "[40000/60000 ( 67%)]  Loss: 0.1420\n",
      "[50000/60000 ( 83%)]  Loss: 0.1752\n",
      "\n",
      "Average test loss: 0.0974  Accuracy: 9682/10000 (96.82%)\n",
      "\n",
      "Epoch: 12\n",
      "[    0/60000 (  0%)]  Loss: 0.1928\n",
      "[10000/60000 ( 17%)]  Loss: 0.1427\n",
      "[20000/60000 ( 33%)]  Loss: 0.1087\n",
      "[30000/60000 ( 50%)]  Loss: 0.2395\n",
      "[40000/60000 ( 67%)]  Loss: 0.2041\n",
      "[50000/60000 ( 83%)]  Loss: 0.0563\n",
      "\n",
      "Average test loss: 0.0779  Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Epoch: 13\n",
      "[    0/60000 (  0%)]  Loss: 0.1704\n",
      "[10000/60000 ( 17%)]  Loss: 0.0668\n",
      "[20000/60000 ( 33%)]  Loss: 0.0393\n",
      "[30000/60000 ( 50%)]  Loss: 0.1994\n",
      "[40000/60000 ( 67%)]  Loss: 0.0398\n",
      "[50000/60000 ( 83%)]  Loss: 0.1770\n",
      "\n",
      "Average test loss: 0.1043  Accuracy: 9704/10000 (97.04%)\n",
      "\n",
      "Epoch: 14\n",
      "[    0/60000 (  0%)]  Loss: 0.0883\n",
      "[10000/60000 ( 17%)]  Loss: 0.0807\n",
      "[20000/60000 ( 33%)]  Loss: 0.0615\n",
      "[30000/60000 ( 50%)]  Loss: 0.0381\n",
      "[40000/60000 ( 67%)]  Loss: 0.1103\n",
      "[50000/60000 ( 83%)]  Loss: 0.1862\n",
      "\n",
      "Average test loss: 0.0802  Accuracy: 9758/10000 (97.58%)\n",
      "\n",
      "Epoch: 15\n",
      "[    0/60000 (  0%)]  Loss: 0.1287\n",
      "[10000/60000 ( 17%)]  Loss: 0.1741\n",
      "[20000/60000 ( 33%)]  Loss: 0.1316\n",
      "[30000/60000 ( 50%)]  Loss: 0.0821\n",
      "[40000/60000 ( 67%)]  Loss: 0.0866\n",
      "[50000/60000 ( 83%)]  Loss: 0.1357\n",
      "\n",
      "Average test loss: 0.0682  Accuracy: 9797/10000 (97.97%)\n",
      "\n",
      "Epoch: 16\n",
      "[    0/60000 (  0%)]  Loss: 0.1409\n",
      "[10000/60000 ( 17%)]  Loss: 0.2012\n",
      "[20000/60000 ( 33%)]  Loss: 0.1478\n",
      "[30000/60000 ( 50%)]  Loss: 0.0520\n",
      "[40000/60000 ( 67%)]  Loss: 0.0945\n",
      "[50000/60000 ( 83%)]  Loss: 0.1473\n",
      "\n",
      "Average test loss: 0.0753  Accuracy: 9758/10000 (97.58%)\n",
      "\n",
      "Epoch: 17\n",
      "[    0/60000 (  0%)]  Loss: 0.0974\n",
      "[10000/60000 ( 17%)]  Loss: 0.0699\n",
      "[20000/60000 ( 33%)]  Loss: 0.1470\n",
      "[30000/60000 ( 50%)]  Loss: 0.1780\n",
      "[40000/60000 ( 67%)]  Loss: 0.0930\n",
      "[50000/60000 ( 83%)]  Loss: 0.0714\n",
      "\n",
      "Average test loss: 0.0822  Accuracy: 9757/10000 (97.57%)\n",
      "\n",
      "Epoch: 18\n",
      "[    0/60000 (  0%)]  Loss: 0.2454\n",
      "[10000/60000 ( 17%)]  Loss: 0.0823\n",
      "[20000/60000 ( 33%)]  Loss: 0.1294\n",
      "[30000/60000 ( 50%)]  Loss: 0.1668\n",
      "[40000/60000 ( 67%)]  Loss: 0.1279\n",
      "[50000/60000 ( 83%)]  Loss: 0.0564\n",
      "\n",
      "Average test loss: 0.0908  Accuracy: 9739/10000 (97.39%)\n",
      "\n",
      "Epoch: 19\n",
      "[    0/60000 (  0%)]  Loss: 0.0593\n",
      "[10000/60000 ( 17%)]  Loss: 0.0677\n",
      "[20000/60000 ( 33%)]  Loss: 0.0592\n",
      "[30000/60000 ( 50%)]  Loss: 0.0342\n",
      "[40000/60000 ( 67%)]  Loss: 0.0734\n",
      "[50000/60000 ( 83%)]  Loss: 0.0691\n",
      "\n",
      "Average test loss: 0.0664  Accuracy: 9799/10000 (97.99%)\n",
      "\n",
      "Epoch: 20\n",
      "[    0/60000 (  0%)]  Loss: 0.1109\n",
      "[10000/60000 ( 17%)]  Loss: 0.1263\n",
      "[20000/60000 ( 33%)]  Loss: 0.1156\n",
      "[30000/60000 ( 50%)]  Loss: 0.0448\n",
      "[40000/60000 ( 67%)]  Loss: 0.1855\n",
      "[50000/60000 ( 83%)]  Loss: 0.0489\n",
      "\n",
      "Average test loss: 0.0709  Accuracy: 9787/10000 (97.87%)\n",
      "\n",
      "Epoch: 21\n",
      "[    0/60000 (  0%)]  Loss: 0.0972\n",
      "[10000/60000 ( 17%)]  Loss: 0.0456\n",
      "[20000/60000 ( 33%)]  Loss: 0.1670\n",
      "[30000/60000 ( 50%)]  Loss: 0.1908\n",
      "[40000/60000 ( 67%)]  Loss: 0.1409\n",
      "[50000/60000 ( 83%)]  Loss: 0.2006\n",
      "\n",
      "Average test loss: 0.0734  Accuracy: 9791/10000 (97.91%)\n",
      "\n",
      "Epoch: 22\n",
      "[    0/60000 (  0%)]  Loss: 0.2003\n",
      "[10000/60000 ( 17%)]  Loss: 0.0880\n",
      "[20000/60000 ( 33%)]  Loss: 0.1480\n",
      "[30000/60000 ( 50%)]  Loss: 0.0563\n",
      "[40000/60000 ( 67%)]  Loss: 0.1238\n",
      "[50000/60000 ( 83%)]  Loss: 0.1423\n",
      "\n",
      "Average test loss: 0.0870  Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Epoch: 23\n",
      "[    0/60000 (  0%)]  Loss: 0.0811\n",
      "[10000/60000 ( 17%)]  Loss: 0.1115\n",
      "[20000/60000 ( 33%)]  Loss: 0.1216\n",
      "[30000/60000 ( 50%)]  Loss: 0.0800\n",
      "[40000/60000 ( 67%)]  Loss: 0.0916\n",
      "[50000/60000 ( 83%)]  Loss: 0.0788\n",
      "\n",
      "Average test loss: 0.0676  Accuracy: 9805/10000 (98.05%)\n",
      "\n",
      "Epoch: 24\n",
      "[    0/60000 (  0%)]  Loss: 0.0972\n",
      "[10000/60000 ( 17%)]  Loss: 0.0308\n",
      "[20000/60000 ( 33%)]  Loss: 0.1709\n",
      "[30000/60000 ( 50%)]  Loss: 0.2387\n",
      "[40000/60000 ( 67%)]  Loss: 0.0711\n",
      "[50000/60000 ( 83%)]  Loss: 0.0497\n",
      "\n",
      "Average test loss: 0.0645  Accuracy: 9829/10000 (98.29%)\n",
      "\n",
      "Epoch: 25\n",
      "[    0/60000 (  0%)]  Loss: 0.1016\n",
      "[10000/60000 ( 17%)]  Loss: 0.0781\n",
      "[20000/60000 ( 33%)]  Loss: 0.0216\n",
      "[30000/60000 ( 50%)]  Loss: 0.0736\n",
      "[40000/60000 ( 67%)]  Loss: 0.0781\n",
      "[50000/60000 ( 83%)]  Loss: 0.1835\n",
      "\n",
      "Average test loss: 0.0689  Accuracy: 9802/10000 (98.02%)\n",
      "\n",
      "Execution time: 3064.43 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS = 25\n",
    "\n",
    "start_time = time.time()\n",
    "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
    "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "train_loss_history, test_loss_history = [], []\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
    "    evaluate(model, test_loader, test_loss_history)\n",
    "\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
